# ğŸ¤– Chat With Gemma

A feature-rich, production-ready **Streamlit application** that lets you chat with Googleâ€™s **Gemma models** (or any Ollama-compatible LLM) in three powerful modes:

| Mode        | Description                                                                 |
|-------------|-----------------------------------------------------------------------------|
| **Chat**    | Classic back-and-forth conversation with memory                             |
| **RAG**     | Upload documents (PDF, TXT, MD) and ask context-aware questions             |
| **WebSearch** | Use DuckDuckGo search results as context for real-time answers             |

---

## ğŸš€ Quick Start

### 1. Install Ollama
ğŸ‘‰ [Download Ollama](https://ollama.ai)  

```bash
ollama pull gemma3:1b
````

### 2. Clone & Install

```bash
git clone https://github.com/your-org/chat-with-gemma.git
cd chat-with-gemma
pip install -r requirements.txt
```

### 3. Launch

```bash
streamlit run app.py
```

App will open at ğŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

## ğŸ”§ Requirements

| Package                  | Purpose                             |
| ------------------------ | ----------------------------------- |
| `streamlit`              | Web UI                              |
| `langchain-ollama`       | LLM integration                     |
| `langchain-community`    | Memory, loaders, FAISS, DuckDuckGo  |
| `sentence-transformers`  | Embeddings                          |
| `pypdf`                  | PDF parsing                         |
| `nltk`                   | Text utilities                      |
| `scikit-learn`           | Similarity metrics                  |
| `plotly`                 | Analytics charts                    |
| `streamlit-ace`          | Code editor (system prompt)         |
| `pdf2image, pytesseract` | OCR for image-based PDFs (optional) |

Install all at once:

```bash
pip install -r requirements.txt
```

---

## ğŸ“ Project Structure

```
chat-with-gemma/
â”œâ”€â”€ app.py                  # Main entry point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ chat_data/              # Auto-created at runtime
    â”œâ”€â”€ faiss_index/        # Vector store snapshots
    â”œâ”€â”€ histories/          # JSON chat exports
    â”œâ”€â”€ cache/              # Temporary downloads
    â”œâ”€â”€ logs/               # chat_app.log
    â””â”€â”€ chat_analytics.db   # SQLite analytics DB
```

---

## âš¡ Key Features

* **Multi-mode Chat** â†’ Chat, RAG, WebSearch
* **Multi-doc RAG** â†’ Upload many PDF/TXT/MD files; chunking + FAISS retrieval
* **OCR Support** â†’ Extract text from scanned PDFs (optional)
* **Confidence Score** â†’ Real-time response quality indicator
* **Response Metrics** â†’ Latency & token-speed tracking
* **Conversation Memory** â†’ Buffer memory with conversation ID isolation
* **Conversation Management** â†’ Multiple saved chats with titles + summaries
* **Search Chat History** â†’ Keyword search across conversations
* **Message Feedback** â†’ ğŸ‘ / ğŸ‘ stored in DB
* **Analytics Dashboard** â†’ 7-day rolling stats (messages, ratio, avg confidence, response time, trends)
* **Modern UI** â†’ Custom glassmorphism, dark/light aware, mobile-friendly
* **System Prompt Presets** â†’ Academic, Creative, Professional one-click personas
* **Import / Export** â†’ Full conversation JSON download & replay
* **Graceful Errors** â†’ User-friendly messages + collapsible stack-traces
* **SQLite Logging** â†’ All messages + metadata for downstream BI
* **Hot-reload Safe** â†’ Session-state hygiene, no duplicate downloads

---

## âš™ï¸ Configuration

All constants live at the top of `app.py`:

```python
OLLAMA_DEFAULT_URL = "http://localhost:11434"
DEFAULT_MODEL        = "gemma3n:e4b"
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
CHUNK_SIZE           = 1000
CHUNK_OVERLAP        = 200
```

Override with env-vars if desired:

```bash
export OLLAMA_URL="http://remote-ollama:11434"
streamlit run app.py
```

---

## ğŸ”’ Security Notes

* Uploaded documents are stored only in RAM and deleted after FAISS index creation.
* No analytics are sent to third parties; everything stays local.
* SQL parameters are used everywhere â†’ no injection risk.
* File-size limit: **10 MB per document** (configurable).

---

## ğŸ› Troubleshooting

| Symptom                         | Fix                                                    |
| ------------------------------- | ------------------------------------------------------ |
| `ImportError: langchain_ollama` | `pip install langchain-ollama`                         |
| `nltk punkt not found`          | Auto-downloaded on first run (quiet mode)              |
| `Connection refused`            | Ensure Ollama service is running on the configured URL |
| PDF fails to load               | `pip install pypdf` and check encoding                 |
| OCR not working                 | Install `pdf2image` and `pytesseract`                  |
| FAISS index empty               | Upload docs â†’ click **Process Documents**              |
| Logs                            | `chat_data/logs/chat_app.log`                          |

---

## ğŸ“œ License

This project is licensed under the **Apache License 2.0**.

---

## ğŸ“¬ Contact

For questions, bug reports, or feature requests:
ğŸ“§ \[[ketanedumail@gmail.com](mailto:ketanedumail@gmail.com)]


Do you also want me to add **badges (Python version, license, Streamlit)** at the top of the README to make it look more like a polished open-source project?
```
